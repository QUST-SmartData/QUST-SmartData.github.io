# SmartData

æ¬¢è¿æ¥åˆ°æˆ‘ä»¬çš„GitHubå›¢é˜Ÿä¸»é¡µï¼

æˆ‘ä»¬å›¢é˜Ÿè‡´åŠ›äºæ¨åŠ¨åœ°è´¨å’ŒåŒ»å­¦ç­‰é¢†åŸŸçš„å›¾åƒåˆ†æç ”ç©¶ï¼Œé€šè¿‡æ·±åº¦å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰æŠ€æœ¯è§£å†³å®é™…é—®é¢˜ã€‚


## ç ”ç©¶æ–¹å‘
æˆ‘ä»¬ä¸“æ³¨äºä»¥ä¸‹ç ”ç©¶æ–¹å‘ï¼š

- ğŸ–¼ï¸ è®¡ç®—æœºè§†è§‰
- ğŸ§  æ·±åº¦å­¦ä¹ 
- ğŸ¤– æœºå™¨å­¦ä¹ 
- ğŸŒ åœ°è´¨å›¾åƒåˆ†æ
- ğŸ¥ åŒ»å­¦å›¾åƒåˆ†æ


## ä»“åº“ä»‹ç»

### åœ°è´¨å­¦ç›¸å…³ç ”ç©¶

è¯¥åˆ†ç±»åŒ…å«ä¸åœ°è´¨å›¾åƒåˆ†æç›¸å…³çš„é¡¹ç›®ï¼Œè‡´åŠ›äºåˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯æå–å’Œåˆ†æåœ°è´¨æ•°æ®ã€‚

#### [FPEM-GAN](https://github.com/QUST-SmartData/FPEM-GAN)

- **æ–¹å‘**ï¼šæµ‹äº•å›¾åƒä¿®å¤

- **è®ºæ–‡æ ‡é¢˜**: *todo*

- **ç®€ä»‹**: todo

- <details>
    <summary>è®ºæ–‡æ‘˜è¦</summary>
    todo
</details>

- **è®ºæ–‡åœ°å€**: åœ¨å®¡


#### [GeoDeepGenerativePrior](https://github.com/QUST-SmartData/GeoDeepGenerativePrior)

- **æ–¹å‘**ï¼šæµ‹äº•å›¾åƒä¿®å¤

- **è®ºæ–‡æ ‡é¢˜**: *todo*

- **ç®€ä»‹**: todo

- <details>
    <summary>è®ºæ–‡æ‘˜è¦</summary>
    todo
</details>

- **è®ºæ–‡åœ°å€**: åœ¨å®¡


#### [ADA-PGGAN](https://github.com/QUST-SmartData/ADA-PGGAN)

- **æ–¹å‘**ï¼š3Dé‡å»º

- **è®ºæ–‡æ ‡é¢˜**: *Multi-scale Reconstruction of 3D Digital Rock by Fusing Critical Information of Fine Scale with Framework of Large Scale: A Novel Approach*

- **ç®€ä»‹**: æˆ‘ä»¬åŸºäºç”Ÿæˆæ¨¡å‹GANæå‡ºäº†ä¸€ç§æ–°é¢–çš„ç‰¹å¾åˆ†å¸ƒå­¦ä¹ å’Œè‡ªé€‚åº”æ•°æ®å¢å¼ºæ¡†æ¶ï¼ŒæˆåŠŸåœ°åœ¨æ•°æ®ç¨€ç¼ºç¯å¢ƒä¸­é‡å»ºé«˜è´¨é‡çš„å¤§è§„æ¨¡ 3D æ•°å­—å²©çŸ³ï¼Œå¹¶é€šè¿‡å¤šç§æŒ‡æ ‡éªŒè¯äº†å…¶å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚

- <details>
    <summary>è®ºæ–‡æ‘˜è¦</summary>
    Amounts of digital rock samples are crucial for studying pore properties. However, it is currently challenging due to equipment limitations or cost considerations. To address this issue, we propose sorts of reconstruction solutions under Data-Scarce Scenarios based on latent inversion predict from proposed generative model. Firstly, A novel featured distribution learning model was proposed though O-ResNet50 network training for prepared inversion. During inversion, the latent vectors predict from mentioned learning model is prepared to interpolate into latent space of given images. To stably produce high-quality images, Adaptive Data Augmentation Progressive Growing Generative Adversarial Network (ADA-PGGAN) is proposed, which includes a mechanism to supervise discriminator's overfitting and automatically adjust levels of data augmentation. Subsequently, interpolated latent vectors are input into the generator to progressively increase image resolution and reconstruct large-scale 3D digital rocks.Finally, evaluations using various metrics were conducted in both 2D and 3D on our results. The Sliced Wasserstein Distance (SWD) was used to assess our proposed data augmentation operation. The majority of SWD values remained below 0.01, with further decreases as resolution increased. Furthermore, generated images accurately exhibited core characteristics.We also evaluated our results in 3D with corresponding metrics, structural properties to indicate consistency with given samples.
</details>

- **è®ºæ–‡åœ°å€**: åœ¨å®¡



#### [DigitalRockConstruction](https://github.com/QUST-SmartData/DigitalRockConstruction)

- **æ–¹å‘**ï¼š3Dé‡å»º

- **è®ºæ–‡æ ‡é¢˜**: *Multi-scale Reconstruction of 3D Digital Rock by Fusing Critical Information of Fine Scale with Framework of Large Scale: A Novel Approach*

- **ç®€ä»‹**: æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆæ¨¡å‹æ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªç½‘ç»œæ•´åˆç²—å°ºåº¦å’Œç²¾ç»†å°ºåº¦ä¿¡æ¯ï¼Œæœ‰æ•ˆåœ°å»ºæ¨¡å¤šå­”ä»‹è´¨çš„å¤šå°ºåº¦ç»“æ„ï¼Œæå‡äº†æ•°å­—å²©çŸ³çš„é‡å»ºæ•ˆç‡å’Œå®ç”¨æ€§ã€‚

- <details>
    <summary>è®ºæ–‡æ‘˜è¦</summary>
    The digital modeling of microstructure is crucial for investigating the physical and transport properties of porous media. Multi-scale modeling of porous media can effectively characterize both coarse-scale and fine-scale information in high-resolution 3D pore structure models with a large field of view (FoV). Currently, there is a lack of comprehensive framework studies on various subscale components that can be integrated with existing scales, such as micropor, mineral-clay, microcracks, etc. To tackle this issue, we propose a novel framework that utilizes combinations of generative models. One of which focuses on predicting coarse-scale structures, while another network fills in fine-scale information to generate combinate-scale structures.
    In the first network, WGAN is selected as basic training network, inputing 3D noises into the generative network and producing images of coarse-scale as output under the supervision of an adversarial network. We make a datasets designed for the adversarial network which only contains coarse-scale images. The other generative network is built for being injected fine-scale information into the coarsescale 3D images generated through the first generator. During the process, we input two-dimensional high-resolution imageswith fine-scale information into the discriminator to generate a multi-scale images. Taking anisotropy into consideration, loss function combinations are presented to deal with. We conduct a case study on a multi-scale digital rock reconstructed of intra-grain pores into inter-grain pores through our approach. Through qualitative and quantitative comparison, it is demonstrated that our method is more practical and efficient than the latest numerical reconstruction methods.
</details>

- **è®ºæ–‡åœ°å€**: åœ¨å®¡


#### [HQJGAN](https://github.com/QUST-SmartData/HQJGAN)

- **æ–¹å‘**ï¼š3DçŸ¿ç‰©åˆ†å‰²

- **è®ºæ–‡æ ‡é¢˜**: *A Novel Workflow of  Segmentation for Finer Mineral Distingished ï¼šAttentionGAN-Swin-Transformer Fused Network*

- **ç®€ä»‹**: æˆ‘ä»¬æå‡ºçš„ DTSS å·¥ä½œæµç»“åˆäº† AttentionGAN å’Œ Swin Transformerï¼Œé€šè¿‡æœ‰æ•ˆçš„åŸŸè½¬æ¢å’Œå›¾åƒåˆ†å‰²ï¼Œå®ç°äº†å¯¹å¤æ‚å²©çŸ³å›¾åƒä¸­çŸ¿ç‰©ç‰¹å¾çš„ç²¾å‡†è¯†åˆ«ã€‚

- <details>
    <summary>è®ºæ–‡æ‘˜è¦</summary>
    We proposed a workflow - DTSS (Domain Transformation and Semantic Segmentation): first use AttentionGAN to convert the CT image domain to the (SEM) scanning electron microscope image domain, and then use Swin Transformer to perform image segmentation. By introducing attention masks and content masks, AttentionGAN can more effectively learn the mapping relationship between the two domains, thereby generating images in the corresponding target domain.
    On the basis of domain transformation, we further use Swin-Transformer for image segmentation. Swin-Transformer is a Transformer-based model that efficiently processes image data through a self-attention mechanism. Compared with traditional convolutional neural networks (CNN), Swin-Transformer's global receptive field and stronger modeling capabilities give it significant advantages when processing complex, multi-mineral rock images. Swin-Transformer is able to capture long-range dependencies in images, which is particularly important for identifying and segmenting mineral dependencies in rocks.
</details>

- **è®ºæ–‡åœ°å€**: åœ¨å®¡


#### [UDAFractureSeg](https://github.com/QUST-SmartData/UDAFractureSeg)

- **æ–¹å‘**ï¼š2Dè£‚ç¼åˆ†å‰²

- **è®ºæ–‡æ ‡é¢˜**: *Unsupervised Domain Adaptation Based Fracture Segmentation Method for Core CT Images*

- **ç®€ä»‹**: æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ— ç›‘ç£åŸŸé€‚åº”çš„é¡µå²©è£‚ç¼è‡ªé€‚åº”åˆ†å‰²æ–¹æ³•ï¼Œç»“åˆé£æ ¼è¿ç§»å’Œåä½œå­¦ä¹ ï¼Œæé«˜äº†åœ¨ä¸åŒåœ°è´¨æ¥æºçš„å²©å¿ƒ CT å›¾åƒä¸­è£‚ç¼åˆ†å‰²çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

- <details>
    <summary>è®ºæ–‡æ‘˜è¦</summary>
    Segmentation of fractures in Computed Tomography (CT) images of cores is crucial in the analysis of rock physical properties. While supervised learning methods have shown significant success in fracture detection, their performance heavily depends on large labeled datasets. However, labeling images is time-consuming and prone to human error. Moreover, these methods often struggle to effectively generalize to unseen datasets due to differences among source and target images. To address this issue, this paper proposes an unsupervised domain-based adaptive segmentation method for shale fractures. The method consists of two parts: StyleFlow-based Style Transfer and Collaborative Learning based Multi-source Domain Adaptation. Firstly, an image style transfer method is introduced to align the images, reducing the difference in gray scale and noise distribution between the source and target domains. Secondly, the Collaborative Learning based Multi-source Domain Adaptation comprises three modules: a segmentation network module, a domain adaptation module, and a collaborative learning module. The segmentation network adopts a modified U-Net with a multi-scale attention mechanism introduced in the encoder part to capture fracture features at different scales in core. Channel and spatial attention mechanisms are also introduced in the decoder part to compensate for loss of spatial structure information caused by downsampling. The domain adaptive module recognizes inter-domain differences and adapts the model with discriminators and adversarial learning to reduce differences in feature or class distributions between source and target domains. The collaborative learning module further corrects unlabeled target domain data using model-generated pseudo-labels, thus improving domain adaptation accuracy. In this way, the segmentation knowledge learned from pavements can be transferred to the core CT image, which enables the adaptive segmentation of core fractures. We conducted experiments on shale datasets from two different geological sources and compared them with existing methods. The results demonstrate that the proposed method exhibits high accuracy and robustness in the segmentation of fractures.
</details>

- **è®ºæ–‡åœ°å€**: åœ¨å®¡


#### [FractureSeg3D](https://github.com/QUST-SmartData/FractureSeg3D)

- **æ–¹å‘**ï¼š3Dè£‚ç¼åˆ†å‰²

- **è®ºæ–‡æ ‡é¢˜**: *Connectivity-Aware 3D Fracture Segmentation Method for Core CT Images*

- **ç®€ä»‹**: æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ 3D å²©å¿ƒ CT å›¾åƒè£‚ç¼åˆ†å‰²æ–¹æ³•ï¼Œåˆ©ç”¨ 3D å¤šå±‚ Transformer ç½‘ç»œæ•è·è¿œç¨‹ä¾èµ–å’Œç©ºé—´è¿ç»­æ€§ç‰¹å¾ï¼Œå¹¶ç»“åˆåŠ¨æ€æƒé‡å’Œå¤šå°ºåº¦ä¸Šä¸‹æ–‡æ„ŸçŸ¥èåˆï¼Œæ˜¾è‘—æå‡äº†å¯¹è£‚ç¼ç©ºé—´ç»“æ„è¿é€šæ€§çš„è¯†åˆ«æ•ˆæœã€‚

- <details>
    <summary>è®ºæ–‡æ‘˜è¦</summary>
    Accurately extracting the fracture structures from three-dimensional (3D) computed tomography (CT) images is essential for simulating and analyzing the physical properties of digital rocks. However, the heterogeneity within the rocks makes it difficult for threshold-based methods to identify blurred fracture boundaries. Furthermore, fractures have a complex spatial topological structure, resulting in existing slice-based segmentation methods ineffective in capturing spatial connectivity information. To address the above problems, a novel fracture segmentation method for 3D core CT images is proposed in this study. Firstly, we introduced a 3D multi-layer Transformer network to capture long-range dependence information and pixel spatial continuity features between adjacent layers.
    Then, we fed three axial slices into a 2D multi-layer Transformer network to extract anisotropic features from multi-views. Subsequently, these features are fed into the Gradient Boosting Decision Tree (GBDT) module, which is iteratively enhanced by weaker learners to obtain preliminary segmentation probability maps. To correct the contribution of these maps to the segmentation results, we add dynamic weights to each of them and adjust it by backpropagation of the loss function. Finally, a multi-scale context-aware fusion module fused spatial continuity features with these maps to obtain segmentation results. We compare it with other state-of-the-art methods and the experiment results demonstrate the superiority of our method in spatial structure connectivity of fracture.
</details>

- **è®ºæ–‡åœ°å€**: åœ¨å®¡


#### [SemSR](https://github.com/QUST-SmartData/SemSR)

- **è®ºæ–‡æ ‡é¢˜**: *Connectivity-Aware 3D Fracture Segmentation Method for Core CT Images*

- **ç®€ä»‹**: æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œç»“åˆè¯­ä¹‰å…±äº«æœºåˆ¶å’Œæ¨¡ç³Šå™ªå£°ï¼Œä»¥æ”¹å–„å­”å£ç‰¹å¾çš„æ¢å¤ï¼Œå…‹æœä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†ä½åˆ†è¾¨ç‡å›¾åƒæ—¶çš„å±€é™æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨å¤æ‚åœ°è´¨ç‰¹å¾ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

- <details>
    <summary>è®ºæ–‡æ‘˜è¦</summary>
    The roughness of pore walls is a crucial factor in studying fluid flow within the pore space. Combining data from different imaging modalities and using deep learning-based super-resolution (SR) methods, a comprehensive view with intricate specific features would be obtained.The relationship between pore wall and pore space is typically representative of geological characterization, which distinguishes among different components. However, current SR methods often overlook geological component regions and incorporate various mechanisms that increase the model's weight and computational demands. To tackle these issues, we employ a Generative Adversarial Network and propose a semantic sharing mechanism to collaborate with the injection of geological characterization. In addition, matching low-resolution (LR) and high-resolution (HR) images is a major challenge. It is common practice to down-sample HR images to obtain pairs of LR images. However, the LR images obtained by these methods still contain lots of details, which weakens the model's generalization ability in real-world scenarios. Therefore, we developed a novel method that introduces intentional blurring noises and multi-sampling operations utilized during data augmentation. Finally, we compare our method with other state-of-the-art methods using proposed indicators to recover the true characteristics of the hole wall, proving the superiority of our method.
</details>

- **è®ºæ–‡åœ°å€**: åœ¨å®¡


### åŒ»å­¦ç›¸å…³ç ”ç©¶
è¯¥åˆ†ç±»åŒ…å«ä¸åŒ»å­¦å›¾åƒå¤„ç†ç›¸å…³çš„é¡¹ç›®ï¼Œä¸“æ³¨äºæé«˜åŒ»å­¦å›¾åƒåˆ†æçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

#### [CXRClassification](https://github.com/QUST-SmartData/CXRClassification)
- è®ºæ–‡æ ‡é¢˜ï¼š*Multi-label Chest X-ray Image Classification Based on Long-range Dependencies Capture and Label Relationships Learning*

- **ç®€ä»‹**: æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆå¤§æ ¸å·ç§¯å’Œå›¾å·ç§¯ç½‘ç»œçš„ CNN æ–¹æ³•ï¼Œé€šè¿‡è§£å‰–åˆ†å‰²å’Œæ ‡ç­¾å…±ç°å…³ç³»ï¼Œæå‡äº†èƒ¸éƒ¨ X å°„çº¿å›¾åƒçš„ç–¾ç—…è¯Šæ–­å‡†ç¡®æ€§ã€‚

- <details>
    <summary>è®ºæ–‡æ‘˜è¦</summary>
    Diagnosing chest diseases from X-ray images using convolutional neural networks (CNNs) is an active area of research. However, existing methods mostly focus on extracting feature information from local regions for prediction, while ignoring the larger-scale image contextual information. Moreover, anatomical segmentation knowledge and co-occurrence relationships among labels, which are important for classification, are not fully utilized. To address the above problems, we proposed a method to capture long-range dependent information in chest X-ray images using a CNN with large kernel convolution. Furthermore, it captures the detailed features of the interest region through anatomical segmentation and builds the potential relationships of different diseases using a graph convolutional network (GCN). Firstly, we pre-trained UNet from a dataset with organ-level annotations for segmenting anatomical regions of interest in the images. Secondly, we build a four-stage backbone network using the large kernel attention (LKA) mechanism and superimpose anatomically segmented regions on the feature maps of each stage to obtain different scales of feature maps for the regions of interest. Thirdly, we utilized a GCN to obtain a co-occurrence matrix representing the potential relationships between all disease labels in the training dataset. Finally, we get the disease diagnosis by combining the label co-occurrence matrix and the visual feature maps. We experimentally show that our proposed method achieves excellent AUC scores of 91.5%, 84.5%, and 82.5% on three publicly available CXR datasetsâ€“NIH, Stanford CheXpert, and MIMIC-CXR-JPG, respectively.
</details>

- **è®ºæ–‡åœ°å€**: åœ¨å®¡


#### [FSTI-GAN](https://github.com/QUST-SmartData/FSTI-GAN)
- è®ºæ–‡æ ‡é¢˜ï¼š*todo*

- **ç®€ä»‹**: todo

- <details>
    <summary>è®ºæ–‡æ‘˜è¦</summary>
    todo
</details>

- **è®ºæ–‡åœ°å€**: åœ¨å®¡


## è¯­è¨€å’Œå·¥å…·

<!-- Your github readme stats
You can use this api: https://github.com/anuraghazra/github-readme-stats
-->
<p>
  <!-- Your languages and tools. Be careful with the alignment. 
  You can use this sites to get logos: https://www.vectorlogo.zone or https://simpleicons.org/
  -->
  <code><img width="30%" src="https://www.vectorlogo.zone/logos/python/python-ar21.svg"></code>
  <code><img width="30%" src="https://www.vectorlogo.zone/logos/numpy/numpy-ar21.svg"></code>
  <code><img width="30%" src="https://www.vectorlogo.zone/logos/pytorch/pytorch-ar21.svg"></code>
  <br />
  <code><img width="30%" src="https://www.vectorlogo.zone/logos/tensorflow/tensorflow-ar21.svg"></code>
  <code><img width="30%" src="https://www.vectorlogo.zone/logos/jupyter/jupyter-ar21.svg"></code>
  <code><img width="30%" src="https://www.vectorlogo.zone/logos/json/json-ar21.svg"></code>
  <br />
  <code><img width="30%" src="https://www.vectorlogo.zone/logos/mysql/mysql-ar21.svg"></code>
  <code><img width="30%" src="https://www.vectorlogo.zone/logos/google_cloud/google_cloud-ar21.svg"></code>
  <code><img width="30%" src="https://www.vectorlogo.zone/logos/docker/docker-ar21.svg"></code>
  <br />
  <code><img width="30%" src="https://www.vectorlogo.zone/logos/git-scm/git-scm-ar21.svg"></code>
  <code><img width="30%" src="https://www.vectorlogo.zone/logos/github/github-ar21.svg"></code>
  <code><img width="30%" src="https://www.vectorlogo.zone/logos/visualstudio_code/visualstudio_code-ar21.svg"></code>
    

</p>


## ç»Ÿè®¡ä¿¡æ¯

<img width="95%" src="https://github-readme-stats.vercel.app/api?username=QUST-SmartData&theme=ambient_gradient&show_icons=true&hide_border=true">

<img width="95%" src="https://github-readme-stats.vercel.app/api/top-langs/?username=QUST-SmartData&hide=jupyter%20notebook&show_icons=true&hide_border=true&layout=donut">

## è”ç³»æˆ‘ä»¬

å¦‚æœ‰é—®é¢˜æˆ–åˆä½œæ„å‘ï¼Œè¯·è”ç³»å›¢é˜Ÿè´Ÿè´£äºº [ç‹é‘«](mailto:lex.wangx@qust.edu.cn) ã€‚
